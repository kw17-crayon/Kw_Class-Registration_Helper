{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "import nltk\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "from torch.nn.utils.rnn import PackedSequence, pack_padded_sequence\n",
    "random.seed(1024)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# CUDA 환경에서 실행시\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "gpus = [0]\n",
    "torch.cuda.set_device(gpus[0])\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU 환경에서 실행시\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "FloatTensor = torch.FloatTensor\n",
    "LongTensor = torch.LongTensor\n",
    "ByteTensor = torch.ByteTensor\n",
    "\n",
    "# train data path\n",
    "path = \"./data/train/[최종]졸업이수학점.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex: eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_batch(batch, w_to_ix): # for bAbI dataset\n",
    "    fact,q,a = list(zip(*batch))\n",
    "    max_fact = max([len(f) for f in fact])\n",
    "    max_len = max([f.size(1) for f in flatten(fact)])\n",
    "    max_q = max([qq.size(1) for qq in q])\n",
    "    max_a = max([aa.size(1) for aa in a])\n",
    "    \n",
    "    facts, fact_masks, q_p, a_p = [], [], [], []\n",
    "    for i in range(len(batch)):\n",
    "        fact_p_t = []\n",
    "        for j in range(len(fact[i])):\n",
    "            if fact[i][j].size(1) < max_len:\n",
    "                fact_p_t.append(torch.cat([fact[i][j], Variable(LongTensor([w_to_ix['<PAD>']] * (max_len - fact[i][j].size(1)))).view(1, -1)], 1))\n",
    "            else:\n",
    "                fact_p_t.append(fact[i][j])\n",
    "\n",
    "        while len(fact_p_t) < max_fact:\n",
    "            fact_p_t.append(Variable(LongTensor([w_to_ix['<PAD>']] * max_len)).view(1, -1))\n",
    "\n",
    "        fact_p_t = torch.cat(fact_p_t)\n",
    "        facts.append(fact_p_t)\n",
    "        fact_masks.append(torch.cat([Variable(ByteTensor(tuple(map(lambda s: s ==0, t.data))), volatile=False) for t in fact_p_t]).view(fact_p_t.size(0), -1))\n",
    "\n",
    "        if q[i].size(1) < max_q:\n",
    "            q_p.append(torch.cat([q[i], Variable(LongTensor([w_to_ix['<PAD>']] * (max_q - q[i].size(1)))).view(1, -1)], 1))\n",
    "        else:\n",
    "            q_p.append(q[i])\n",
    "\n",
    "        if a[i].size(1) < max_a:\n",
    "            a_p.append(torch.cat([a[i], Variable(LongTensor([w_to_ix['<PAD>']] * (max_a - a[i].size(1)))).view(1, -1)], 1))\n",
    "        else:\n",
    "            a_p.append(a[i])\n",
    "\n",
    "    questions = torch.cat(q_p)\n",
    "    answers = torch.cat(a_p)\n",
    "    question_masks = torch.cat([Variable(ByteTensor(tuple(map(lambda s: s ==0, t.data))), volatile=False) for t in questions]).view(questions.size(0), -1)\n",
    "    \n",
    "    return facts, fact_masks, questions, question_masks, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_index):\n",
    "    idxs = list(map(lambda w: to_index[w] if to_index.get(w) is not None else to_index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data text파일을 읽어서 팩트, 질문 분할 및 저장하여 반환하는 코드\n",
    "def bAbI_data_load(path):\n",
    "    try:\n",
    "        data = open(path, 'r' ,encoding='utf8').readlines()\n",
    "    except:\n",
    "        print(\"Such a file does not exist at %s\".format(path))\n",
    "        return None\n",
    "    \n",
    "    data = [d[:-1] for d in data]\n",
    "    data_p = []\n",
    "    fact = []\n",
    "    qa = []\n",
    "    try:\n",
    "        for d in data:\n",
    "            index = d.split(' ')[0]\n",
    "            if index == '1':\n",
    "                fact = []\n",
    "                qa = []\n",
    "            if '?' in d:\n",
    "                temp = d.split('\\t')\n",
    "                q = temp[0].strip().replace('?', '').split(' ')[1:] + ['?']\n",
    "                a = temp[1].split() + ['</s>']\n",
    "                stemp = deepcopy(fact)\n",
    "                data_p.append([stemp, q, a])\n",
    "            else:\n",
    "                tokens = d.replace('.', '').split(' ')[1:] + ['</s>']\n",
    "                fact.append(tokens)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Please check the data is right\")\n",
    "        return None\n",
    "    return data_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = bAbI_data_load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['17학번', '영어회화', '필수', '안', '들어도', '돼', '</s>'],\n",
       "  ['18학번', '영어회화', '필수', '안', '들어도', '돼', '</s>'],\n",
       "  ['17학번', '대학영어', '영어레벨테스트', '통과하지', '못한', '사람만', '들으면', '돼', '</s>'],\n",
       "  ['18학번', '대학영어', '영어레벨테스트', '통과하지', '못한', '사람만', '들으면', '돼', '</s>'],\n",
       "  ['필수교양,', '균형교양,', '기초교양', '', '들어야', '해', '</s>'],\n",
       "  ['17학번', '필수교양', '', '광운인되기,', '영어,', '정보가', '있어', '</s>'],\n",
       "  ['18학번', '필수교양', '', '광운인되기,', '영어,', '정보가', '있어', '</s>'],\n",
       "  ['19학번', '필수교양', '', '광운인되기,', '영어,', '정보가', '있어', '</s>'],\n",
       "  ['균형교양', '3학점', '과목만', '인정돼', '</s>'],\n",
       "  ['19학번', '대학영어', '', '필수', '', '들어야', '돼', '</s>'],\n",
       "  ['20학번', '필수교양', '', '광운인되기,', '대학영어,', '정보,', '융합적사고와글쓰기가', '있어', '</s>'],\n",
       "  ['20학번', '균형교양', '', '5영역', '중', '3영역x3학점', '', '의무', '이수해야', '해', '</s>'],\n",
       "  ['17학번', '졸업', '이수학점', '133학점', '야', '</s>'],\n",
       "  ['18학번', '졸업', '이수학점', '133학점', '야', '</s>'],\n",
       "  ['17학번', '필수+균형', '교양', '', '19~22학점', '들어야', '해', '</s>'],\n",
       "  ['18학번', '필수+균형', '교양', '', '19~22학점', '들어야', '해', '</s>'],\n",
       "  ['컴퓨터정보공학부', '17학번', '', '기초교양', '', '24학점', '들어야', '해', '</s>'],\n",
       "  ['컴퓨터정보공학부', '18학번', '', '기초교양', '', '24학점', '들어야', '해', '</s>'],\n",
       "  ['컴퓨터정보공학부', '19학번', '', '기초교양', '', '24학점', '들어야', '해', '</s>'],\n",
       "  ['소프트웨어학부', '17학번', '', '기초교양', '', '21학점', '들어야', '해', '</s>'],\n",
       "  ['소프트웨어학부', '18학번', '', '기초교양', '', '21학점', '들어야', '해', '</s>'],\n",
       "  ['소프트웨어학부', '19학번', '', '기초교양', '', '21학점', '들어야', '해', '</s>'],\n",
       "  ['정보융합학부', '17학번', '', '기초교양', '', '9학점', '들어야', '해', '</s>'],\n",
       "  ['정보융합학부', '18학번', '', '기초교양', '', '9학점', '들어야', '해', '</s>'],\n",
       "  ['정보융합학부', '19학번', '', '기초교양', '', '9학점', '들어야', '해', '</s>'],\n",
       "  ['19학번', '', '필수+균형', '교양', '', '22학점', '들어야', '해', '</s>'],\n",
       "  ['20학번', '', '필수+균형', '교양', '', '22학점', '들어야', '해', '</s>'],\n",
       "  ['컴퓨터정보공학부', '20학번', '', '기초교양', '', '27학점', '들어야', '해', '</s>'],\n",
       "  ['소프트웨어학부', '20학번', '', '기초교양', '', '24학점', '들어야', '해', '</s>'],\n",
       "  ['정보융합학부', '20학번', '', '기초교양', '', '9학점', '들어야', '해', '</s>']],\n",
       " ['17학번', '', '영어회화', '', '필수', '', '들어야', '해', '', '', '?'],\n",
       " ['아니', '</s>']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data 문장들을 쪼개서 vaca를 dict 형태로 저장하기 위한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact,q,a = list(zip(*train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(flatten(flatten(fact)) + flatten(q) + flatten(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index={'<PAD>': 0, '<UNK>': 1, '<s>': 2, '</s>': 3}\n",
    "for vo in vocab:\n",
    "    if word2index.get(vo) is None:\n",
    "        word2index[vo] = len(word2index)\n",
    "index2word = {v:k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in train_data:\n",
    "    for i,fact in enumerate(t[0]):\n",
    "        t[0][i] = prepare_sequence(fact, word2index).view(1, -1)\n",
    "    \n",
    "    t[1] = prepare_sequence(t[1], word2index).view(1, -1)\n",
    "    t[2] = prepare_sequence(t[2], word2index).view(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DMN Model 코드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(DMN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(input_size, hidden_size, padding_idx=0) #sparse=True)\n",
    "        self.input_gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.question_gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        self.gate = nn.Sequential(\n",
    "                            nn.Linear(hidden_size * 4, hidden_size),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(hidden_size, 1),\n",
    "                            nn.Sigmoid()\n",
    "                        )\n",
    "        \n",
    "        self.attention_grucell =  nn.GRUCell(hidden_size, hidden_size)\n",
    "        self.memory_grucell = nn.GRUCell(hidden_size, hidden_size)\n",
    "        self.answer_grucell = nn.GRUCell(hidden_size * 2, hidden_size)\n",
    "        self.answer_fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "    def init_hidden(self, inputs):\n",
    "        hidden = Variable(torch.zeros(1, inputs.size(0), self.hidden_size))\n",
    "        return hidden.cuda() if USE_CUDA else hidden\n",
    "    \n",
    "    def init_weight(self):\n",
    "        nn.init.xavier_uniform(self.embed.state_dict()['weight'])\n",
    "        \n",
    "        for name, param in self.input_gru.state_dict().items():\n",
    "            if 'weight' in name: nn.init.xavier_normal(param)\n",
    "        for name, param in self.question_gru.state_dict().items():\n",
    "            if 'weight' in name: nn.init.xavier_normal(param)\n",
    "        for name, param in self.gate.state_dict().items():\n",
    "            if 'weight' in name: nn.init.xavier_normal(param)\n",
    "        for name, param in self.attention_grucell.state_dict().items():\n",
    "            if 'weight' in name: nn.init.xavier_normal(param)\n",
    "        for name, param in self.memory_grucell.state_dict().items():\n",
    "            if 'weight' in name: nn.init.xavier_normal(param)\n",
    "        for name, param in self.answer_grucell.state_dict().items():\n",
    "            if 'weight' in name: nn.init.xavier_normal(param)\n",
    "        \n",
    "        nn.init.xavier_normal(self.answer_fc.state_dict()['weight'])\n",
    "        self.answer_fc.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, facts, fact_masks, questions, question_masks, num_decode, episodes=3, is_training=False):\n",
    "        \"\"\"\n",
    "        facts : (B,T_C,T_I) / LongTensor in List # batch_size, num_of_facts, length_of_each_fact(padded)\n",
    "        fact_masks : (B,T_C,T_I) / ByteTensor in List # batch_size, num_of_facts, length_of_each_fact(padded)\n",
    "        questions : (B,T_Q) / LongTensor # batch_size, question_length\n",
    "        question_masks : (B,T_Q) / ByteTensor # batch_size, question_length\n",
    "        \"\"\"\n",
    "        # Input Module\n",
    "        C = [] # encoded facts\n",
    "        for fact, fact_mask in zip(facts, fact_masks):\n",
    "            embeds = self.embed(fact)\n",
    "            if is_training:\n",
    "                embeds = self.dropout(embeds)\n",
    "            hidden = self.init_hidden(fact)\n",
    "            outputs, hidden = self.input_gru(embeds, hidden)\n",
    "            real_hidden = []\n",
    "\n",
    "            for i, o in enumerate(outputs): # B,T,D\n",
    "                real_length = fact_mask[i].data.tolist().count(0) \n",
    "                real_hidden.append(o[real_length - 1])\n",
    "\n",
    "            C.append(torch.cat(real_hidden).view(fact.size(0), -1).unsqueeze(0))\n",
    "        \n",
    "        encoded_facts = torch.cat(C) # B,T_C,D\n",
    "        \n",
    "        # Question Module\n",
    "        embeds = self.embed(questions)\n",
    "        if is_training:\n",
    "            embeds = self.dropout(embeds)\n",
    "        hidden = self.init_hidden(questions)\n",
    "        outputs, hidden = self.question_gru(embeds, hidden)\n",
    "        \n",
    "        if isinstance(question_masks, torch.autograd.Variable):\n",
    "            real_question = []\n",
    "            for i, o in enumerate(outputs): # B,T,D\n",
    "                real_length = question_masks[i].data.tolist().count(0) \n",
    "                real_question.append(o[real_length - 1])\n",
    "            encoded_question = torch.cat(real_question).view(questions.size(0), -1) # B,D\n",
    "        else: # for inference mode\n",
    "            encoded_question = hidden.squeeze(0) # B,D\n",
    "            \n",
    "        # Episodic Memory Module\n",
    "        memory = encoded_question\n",
    "        T_C = encoded_facts.size(1)\n",
    "        B = encoded_facts.size(0)\n",
    "        for i in range(episodes):\n",
    "            hidden = self.init_hidden(encoded_facts.transpose(0, 1)[0]).squeeze(0) # B,D\n",
    "            for t in range(T_C):\n",
    "                #TODO: fact masking\n",
    "                #TODO: gate function => softmax\n",
    "                z = torch.cat([\n",
    "                                    encoded_facts.transpose(0, 1)[t] * encoded_question, # B,D , element-wise product\n",
    "                                    encoded_facts.transpose(0, 1)[t] * memory, # B,D , element-wise product\n",
    "                                    torch.abs(encoded_facts.transpose(0,1)[t] - encoded_question), # B,D\n",
    "                                    torch.abs(encoded_facts.transpose(0,1)[t] - memory) # B,D\n",
    "                                ], 1)\n",
    "                g_t = self.gate(z) # B,1 scalar\n",
    "                hidden = g_t * self.attention_grucell(encoded_facts.transpose(0, 1)[t], hidden) + (1 - g_t) * hidden\n",
    "                \n",
    "            e = hidden\n",
    "            memory = self.memory_grucell(e, memory)\n",
    "        \n",
    "        # Answer Module\n",
    "        answer_hidden = memory\n",
    "        start_decode = Variable(LongTensor([[word2index['<s>']] * memory.size(0)])).transpose(0, 1)\n",
    "        y_t_1 = self.embed(start_decode).squeeze(1) # B,D\n",
    "        \n",
    "        decodes = []\n",
    "        for t in range(num_decode):\n",
    "            answer_hidden = self.answer_grucell(torch.cat([y_t_1, encoded_question], 1), answer_hidden)\n",
    "            decodes.append(F.log_softmax(self.answer_fc(answer_hidden),1))\n",
    "        return torch.cat(decodes, 1).view(B * num_decode, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델의 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 80\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.001\n",
    "EPOCH = 10000\n",
    "NUM_EPISODE = 3\n",
    "EARLY_STOPPING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-23f920493938>:29: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(self.embed.state_dict()['weight'])\n",
      "<ipython-input-19-23f920493938>:32: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  if 'weight' in name: nn.init.xavier_normal(param)\n",
      "<ipython-input-19-23f920493938>:34: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  if 'weight' in name: nn.init.xavier_normal(param)\n",
      "<ipython-input-19-23f920493938>:36: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  if 'weight' in name: nn.init.xavier_normal(param)\n",
      "<ipython-input-19-23f920493938>:38: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  if 'weight' in name: nn.init.xavier_normal(param)\n",
      "<ipython-input-19-23f920493938>:40: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  if 'weight' in name: nn.init.xavier_normal(param)\n",
      "<ipython-input-19-23f920493938>:42: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  if 'weight' in name: nn.init.xavier_normal(param)\n",
      "<ipython-input-19-23f920493938>:44: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  nn.init.xavier_normal(self.answer_fc.state_dict()['weight'])\n"
     ]
    }
   ],
   "source": [
    "# 모델 선언\n",
    "model = DMN(len(word2index), HIDDEN_SIZE, len(word2index))\n",
    "model.init_weight()\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "# loss 함수와 최적화 함수 선언\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10000] mean_loss : 4.36\n",
      "[1/10000] mean_loss : 4.27\n",
      "[2/10000] mean_loss : 4.19\n",
      "[3/10000] mean_loss : 4.10\n",
      "[4/10000] mean_loss : 4.01\n",
      "[5/10000] mean_loss : 3.90\n",
      "[6/10000] mean_loss : 3.78\n",
      "[7/10000] mean_loss : 3.65\n",
      "[8/10000] mean_loss : 3.50\n",
      "[9/10000] mean_loss : 3.34\n",
      "[10/10000] mean_loss : 3.17\n",
      "[11/10000] mean_loss : 3.01\n",
      "[12/10000] mean_loss : 2.89\n",
      "[13/10000] mean_loss : 2.79\n",
      "[14/10000] mean_loss : 2.73\n",
      "[15/10000] mean_loss : 2.68\n",
      "[16/10000] mean_loss : 2.62\n",
      "[17/10000] mean_loss : 2.54\n",
      "[18/10000] mean_loss : 2.47\n",
      "[19/10000] mean_loss : 2.41\n",
      "[20/10000] mean_loss : 2.35\n",
      "[21/10000] mean_loss : 2.31\n",
      "[22/10000] mean_loss : 2.29\n",
      "[23/10000] mean_loss : 2.26\n",
      "[24/10000] mean_loss : 2.24\n",
      "[25/10000] mean_loss : 2.21\n",
      "[26/10000] mean_loss : 2.17\n",
      "[27/10000] mean_loss : 2.14\n",
      "[28/10000] mean_loss : 2.11\n",
      "[29/10000] mean_loss : 2.09\n",
      "[30/10000] mean_loss : 2.06\n",
      "[31/10000] mean_loss : 2.05\n",
      "[32/10000] mean_loss : 2.03\n",
      "[33/10000] mean_loss : 2.01\n",
      "[34/10000] mean_loss : 1.99\n",
      "[35/10000] mean_loss : 1.97\n",
      "[36/10000] mean_loss : 1.95\n",
      "[37/10000] mean_loss : 1.93\n",
      "[38/10000] mean_loss : 1.90\n",
      "[39/10000] mean_loss : 1.88\n",
      "[40/10000] mean_loss : 1.86\n",
      "[41/10000] mean_loss : 1.85\n",
      "[42/10000] mean_loss : 1.83\n",
      "[43/10000] mean_loss : 1.81\n",
      "[44/10000] mean_loss : 1.79\n",
      "[45/10000] mean_loss : 1.77\n",
      "[46/10000] mean_loss : 1.75\n",
      "[47/10000] mean_loss : 1.73\n",
      "[48/10000] mean_loss : 1.71\n",
      "[49/10000] mean_loss : 1.68\n",
      "[50/10000] mean_loss : 1.67\n",
      "[51/10000] mean_loss : 1.64\n",
      "[52/10000] mean_loss : 1.62\n",
      "[53/10000] mean_loss : 1.60\n",
      "[54/10000] mean_loss : 1.58\n",
      "[55/10000] mean_loss : 1.56\n",
      "[56/10000] mean_loss : 1.54\n",
      "[57/10000] mean_loss : 1.53\n",
      "[58/10000] mean_loss : 1.51\n",
      "[59/10000] mean_loss : 1.49\n",
      "[60/10000] mean_loss : 1.47\n",
      "[61/10000] mean_loss : 1.46\n",
      "[62/10000] mean_loss : 1.44\n",
      "[63/10000] mean_loss : 1.42\n",
      "[64/10000] mean_loss : 1.40\n",
      "[65/10000] mean_loss : 1.39\n",
      "[66/10000] mean_loss : 1.37\n",
      "[67/10000] mean_loss : 1.36\n",
      "[68/10000] mean_loss : 1.34\n",
      "[69/10000] mean_loss : 1.33\n",
      "[70/10000] mean_loss : 1.31\n",
      "[71/10000] mean_loss : 1.30\n",
      "[72/10000] mean_loss : 1.28\n",
      "[73/10000] mean_loss : 1.27\n",
      "[74/10000] mean_loss : 1.25\n",
      "[75/10000] mean_loss : 1.24\n",
      "[76/10000] mean_loss : 1.23\n",
      "[77/10000] mean_loss : 1.22\n",
      "[78/10000] mean_loss : 1.20\n",
      "[79/10000] mean_loss : 1.19\n",
      "[80/10000] mean_loss : 1.17\n",
      "[81/10000] mean_loss : 1.16\n",
      "[82/10000] mean_loss : 1.15\n",
      "[83/10000] mean_loss : 1.14\n",
      "[84/10000] mean_loss : 1.12\n",
      "[85/10000] mean_loss : 1.11\n",
      "[86/10000] mean_loss : 1.10\n",
      "[87/10000] mean_loss : 1.08\n",
      "[88/10000] mean_loss : 1.07\n",
      "[89/10000] mean_loss : 1.06\n",
      "[90/10000] mean_loss : 1.05\n",
      "[91/10000] mean_loss : 1.04\n",
      "[92/10000] mean_loss : 1.03\n",
      "[93/10000] mean_loss : 1.02\n",
      "[94/10000] mean_loss : 1.01\n",
      "[95/10000] mean_loss : 0.99\n",
      "[96/10000] mean_loss : 0.98\n",
      "[97/10000] mean_loss : 0.98\n",
      "[98/10000] mean_loss : 0.96\n",
      "[99/10000] mean_loss : 0.95\n",
      "[100/10000] mean_loss : 0.94\n",
      "[101/10000] mean_loss : 0.93\n",
      "[102/10000] mean_loss : 0.92\n",
      "[103/10000] mean_loss : 0.91\n",
      "[104/10000] mean_loss : 0.90\n",
      "[105/10000] mean_loss : 0.90\n",
      "[106/10000] mean_loss : 0.88\n",
      "[107/10000] mean_loss : 0.86\n",
      "[108/10000] mean_loss : 0.86\n",
      "[109/10000] mean_loss : 0.85\n",
      "[110/10000] mean_loss : 0.84\n",
      "[111/10000] mean_loss : 0.83\n",
      "[112/10000] mean_loss : 0.82\n",
      "[113/10000] mean_loss : 0.80\n",
      "[114/10000] mean_loss : 0.79\n",
      "[115/10000] mean_loss : 0.78\n",
      "[116/10000] mean_loss : 0.77\n",
      "[117/10000] mean_loss : 0.78\n",
      "[118/10000] mean_loss : 0.74\n",
      "[119/10000] mean_loss : 0.73\n",
      "[120/10000] mean_loss : 0.73\n",
      "[121/10000] mean_loss : 0.71\n",
      "[122/10000] mean_loss : 0.69\n",
      "[123/10000] mean_loss : 0.69\n",
      "[124/10000] mean_loss : 0.68\n",
      "[125/10000] mean_loss : 0.68\n",
      "[126/10000] mean_loss : 0.66\n",
      "[127/10000] mean_loss : 0.66\n",
      "[128/10000] mean_loss : 0.64\n",
      "[129/10000] mean_loss : 0.63\n",
      "[130/10000] mean_loss : 0.63\n",
      "[131/10000] mean_loss : 0.62\n",
      "[132/10000] mean_loss : 0.61\n",
      "[133/10000] mean_loss : 0.60\n",
      "[134/10000] mean_loss : 0.58\n",
      "[135/10000] mean_loss : 0.59\n",
      "[136/10000] mean_loss : 0.58\n",
      "[137/10000] mean_loss : 0.56\n",
      "[138/10000] mean_loss : 0.56\n",
      "[139/10000] mean_loss : 0.55\n",
      "[140/10000] mean_loss : 0.54\n",
      "[141/10000] mean_loss : 0.53\n",
      "[142/10000] mean_loss : 0.53\n",
      "[143/10000] mean_loss : 0.53\n",
      "[144/10000] mean_loss : 0.52\n",
      "[145/10000] mean_loss : 0.50\n",
      "[146/10000] mean_loss : 0.50\n",
      "[147/10000] mean_loss : 0.49\n",
      "[148/10000] mean_loss : 0.48\n",
      "[149/10000] mean_loss : 0.49\n",
      "[150/10000] mean_loss : 0.49\n",
      "[151/10000] mean_loss : 0.47\n",
      "[152/10000] mean_loss : 0.46\n",
      "[153/10000] mean_loss : 0.45\n",
      "[154/10000] mean_loss : 0.46\n",
      "[155/10000] mean_loss : 0.45\n",
      "[156/10000] mean_loss : 0.43\n",
      "[157/10000] mean_loss : 0.43\n",
      "[158/10000] mean_loss : 0.44\n",
      "[159/10000] mean_loss : 0.42\n",
      "[160/10000] mean_loss : 0.42\n",
      "[161/10000] mean_loss : 0.41\n",
      "[162/10000] mean_loss : 0.41\n",
      "[163/10000] mean_loss : 0.39\n",
      "[164/10000] mean_loss : 0.40\n",
      "[165/10000] mean_loss : 0.39\n",
      "[166/10000] mean_loss : 0.38\n",
      "[167/10000] mean_loss : 0.38\n",
      "[168/10000] mean_loss : 0.37\n",
      "[169/10000] mean_loss : 0.36\n",
      "[170/10000] mean_loss : 0.35\n",
      "[171/10000] mean_loss : 0.35\n",
      "[172/10000] mean_loss : 0.35\n",
      "[173/10000] mean_loss : 0.38\n",
      "[174/10000] mean_loss : 0.33\n",
      "[175/10000] mean_loss : 0.32\n",
      "[176/10000] mean_loss : 0.33\n",
      "[177/10000] mean_loss : 0.33\n",
      "[178/10000] mean_loss : 0.32\n",
      "[179/10000] mean_loss : 0.31\n",
      "[180/10000] mean_loss : 0.30\n",
      "[181/10000] mean_loss : 0.30\n",
      "[182/10000] mean_loss : 0.28\n",
      "[183/10000] mean_loss : 0.28\n",
      "[184/10000] mean_loss : 0.29\n",
      "[185/10000] mean_loss : 0.28\n",
      "[186/10000] mean_loss : 0.27\n",
      "[187/10000] mean_loss : 0.26\n",
      "[188/10000] mean_loss : 0.26\n",
      "[189/10000] mean_loss : 0.25\n",
      "[190/10000] mean_loss : 0.27\n",
      "[191/10000] mean_loss : 0.24\n",
      "[192/10000] mean_loss : 0.24\n",
      "[193/10000] mean_loss : 0.23\n",
      "[194/10000] mean_loss : 0.23\n",
      "[195/10000] mean_loss : 0.23\n",
      "[196/10000] mean_loss : 0.22\n",
      "[197/10000] mean_loss : 0.21\n",
      "[198/10000] mean_loss : 0.21\n",
      "[199/10000] mean_loss : 0.21\n",
      "[200/10000] mean_loss : 0.20\n",
      "[201/10000] mean_loss : 0.20\n",
      "[202/10000] mean_loss : 0.19\n",
      "[203/10000] mean_loss : 0.19\n",
      "[204/10000] mean_loss : 0.19\n",
      "[205/10000] mean_loss : 0.18\n",
      "[206/10000] mean_loss : 0.17\n",
      "[207/10000] mean_loss : 0.17\n",
      "[208/10000] mean_loss : 0.16\n",
      "[209/10000] mean_loss : 0.16\n",
      "[210/10000] mean_loss : 0.16\n",
      "[211/10000] mean_loss : 0.15\n",
      "[212/10000] mean_loss : 0.15\n",
      "[213/10000] mean_loss : 0.14\n",
      "[214/10000] mean_loss : 0.14\n",
      "[215/10000] mean_loss : 0.14\n",
      "[216/10000] mean_loss : 0.14\n",
      "[217/10000] mean_loss : 0.13\n",
      "[218/10000] mean_loss : 0.13\n",
      "[219/10000] mean_loss : 0.13\n",
      "[220/10000] mean_loss : 0.13\n",
      "[221/10000] mean_loss : 0.12\n",
      "[222/10000] mean_loss : 0.12\n",
      "[223/10000] mean_loss : 0.11\n",
      "[224/10000] mean_loss : 0.12\n",
      "[225/10000] mean_loss : 0.11\n",
      "[226/10000] mean_loss : 0.11\n",
      "[227/10000] mean_loss : 0.10\n",
      "[228/10000] mean_loss : 0.10\n",
      "[229/10000] mean_loss : 0.10\n",
      "[230/10000] mean_loss : 0.10\n",
      "[231/10000] mean_loss : 0.10\n",
      "[232/10000] mean_loss : 0.09\n",
      "[233/10000] mean_loss : 0.09\n",
      "[234/10000] mean_loss : 0.09\n",
      "[235/10000] mean_loss : 0.09\n",
      "[236/10000] mean_loss : 0.08\n",
      "[237/10000] mean_loss : 0.08\n",
      "[238/10000] mean_loss : 0.08\n",
      "[239/10000] mean_loss : 0.08\n",
      "[240/10000] mean_loss : 0.08\n",
      "[241/10000] mean_loss : 0.08\n",
      "[242/10000] mean_loss : 0.07\n",
      "[243/10000] mean_loss : 0.07\n",
      "[244/10000] mean_loss : 0.07\n",
      "[245/10000] mean_loss : 0.07\n",
      "[246/10000] mean_loss : 0.07\n",
      "[247/10000] mean_loss : 0.07\n",
      "[248/10000] mean_loss : 0.07\n",
      "[249/10000] mean_loss : 0.07\n",
      "[250/10000] mean_loss : 0.07\n",
      "[251/10000] mean_loss : 0.06\n",
      "[252/10000] mean_loss : 0.06\n",
      "[253/10000] mean_loss : 0.06\n",
      "[254/10000] mean_loss : 0.06\n",
      "[255/10000] mean_loss : 0.06\n",
      "[256/10000] mean_loss : 0.06\n",
      "[257/10000] mean_loss : 0.06\n",
      "[258/10000] mean_loss : 0.06\n",
      "[259/10000] mean_loss : 0.05\n",
      "[260/10000] mean_loss : 0.05\n",
      "[261/10000] mean_loss : 0.05\n",
      "[262/10000] mean_loss : 0.05\n",
      "[263/10000] mean_loss : 0.05\n",
      "[264/10000] mean_loss : 0.05\n",
      "[265/10000] mean_loss : 0.05\n",
      "[266/10000] mean_loss : 0.05\n",
      "[267/10000] mean_loss : 0.05\n",
      "[268/10000] mean_loss : 0.05\n",
      "[269/10000] mean_loss : 0.05\n",
      "[270/10000] mean_loss : 0.05\n",
      "[271/10000] mean_loss : 0.04\n",
      "[272/10000] mean_loss : 0.05\n",
      "[273/10000] mean_loss : 0.04\n",
      "[274/10000] mean_loss : 0.04\n",
      "[275/10000] mean_loss : 0.04\n",
      "[276/10000] mean_loss : 0.04\n",
      "[277/10000] mean_loss : 0.04\n",
      "[278/10000] mean_loss : 0.04\n",
      "[279/10000] mean_loss : 0.04\n",
      "[280/10000] mean_loss : 0.04\n",
      "[281/10000] mean_loss : 0.04\n",
      "[282/10000] mean_loss : 0.04\n",
      "[283/10000] mean_loss : 0.04\n",
      "[284/10000] mean_loss : 0.04\n",
      "[285/10000] mean_loss : 0.04\n",
      "[286/10000] mean_loss : 0.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[287/10000] mean_loss : 0.04\n",
      "[288/10000] mean_loss : 0.04\n",
      "[289/10000] mean_loss : 0.04\n",
      "[290/10000] mean_loss : 0.03\n",
      "[291/10000] mean_loss : 0.04\n",
      "[292/10000] mean_loss : 0.03\n",
      "[293/10000] mean_loss : 0.03\n",
      "[294/10000] mean_loss : 0.03\n",
      "[295/10000] mean_loss : 0.03\n",
      "[296/10000] mean_loss : 0.03\n",
      "[297/10000] mean_loss : 0.03\n",
      "[298/10000] mean_loss : 0.03\n",
      "[299/10000] mean_loss : 0.03\n",
      "[300/10000] mean_loss : 0.03\n",
      "[301/10000] mean_loss : 0.03\n",
      "[302/10000] mean_loss : 0.03\n",
      "[303/10000] mean_loss : 0.03\n",
      "[304/10000] mean_loss : 0.03\n",
      "[305/10000] mean_loss : 0.03\n",
      "[306/10000] mean_loss : 0.03\n",
      "[307/10000] mean_loss : 0.03\n",
      "[308/10000] mean_loss : 0.03\n",
      "[309/10000] mean_loss : 0.03\n",
      "[310/10000] mean_loss : 0.03\n",
      "[311/10000] mean_loss : 0.03\n",
      "[312/10000] mean_loss : 0.03\n",
      "[313/10000] mean_loss : 0.03\n",
      "[314/10000] mean_loss : 0.03\n",
      "[315/10000] mean_loss : 0.03\n",
      "[316/10000] mean_loss : 0.03\n",
      "[317/10000] mean_loss : 0.03\n",
      "[318/10000] mean_loss : 0.03\n",
      "[319/10000] mean_loss : 0.03\n",
      "[320/10000] mean_loss : 0.02\n",
      "[321/10000] mean_loss : 0.02\n",
      "[322/10000] mean_loss : 0.02\n",
      "[323/10000] mean_loss : 0.02\n",
      "[324/10000] mean_loss : 0.02\n",
      "[325/10000] mean_loss : 0.02\n",
      "[326/10000] mean_loss : 0.02\n",
      "[327/10000] mean_loss : 0.02\n",
      "[328/10000] mean_loss : 0.02\n",
      "[329/10000] mean_loss : 0.02\n",
      "[330/10000] mean_loss : 0.02\n",
      "[331/10000] mean_loss : 0.02\n",
      "[332/10000] mean_loss : 0.02\n",
      "[333/10000] mean_loss : 0.02\n",
      "[334/10000] mean_loss : 0.02\n",
      "[335/10000] mean_loss : 0.02\n",
      "[336/10000] mean_loss : 0.02\n",
      "[337/10000] mean_loss : 0.02\n",
      "[338/10000] mean_loss : 0.02\n",
      "[339/10000] mean_loss : 0.02\n",
      "[340/10000] mean_loss : 0.02\n",
      "[341/10000] mean_loss : 0.02\n",
      "[342/10000] mean_loss : 0.02\n",
      "[343/10000] mean_loss : 0.02\n",
      "[344/10000] mean_loss : 0.02\n",
      "[345/10000] mean_loss : 0.02\n",
      "[346/10000] mean_loss : 0.02\n",
      "[347/10000] mean_loss : 0.02\n",
      "[348/10000] mean_loss : 0.02\n",
      "[349/10000] mean_loss : 0.02\n",
      "[350/10000] mean_loss : 0.02\n",
      "[351/10000] mean_loss : 0.02\n",
      "[352/10000] mean_loss : 0.02\n",
      "[353/10000] mean_loss : 0.02\n",
      "[354/10000] mean_loss : 0.02\n",
      "[355/10000] mean_loss : 0.02\n",
      "[356/10000] mean_loss : 0.02\n",
      "[357/10000] mean_loss : 0.02\n",
      "[358/10000] mean_loss : 0.02\n",
      "[359/10000] mean_loss : 0.02\n",
      "[360/10000] mean_loss : 0.02\n",
      "[361/10000] mean_loss : 0.02\n",
      "[362/10000] mean_loss : 0.02\n",
      "[363/10000] mean_loss : 0.02\n",
      "[364/10000] mean_loss : 0.02\n",
      "[365/10000] mean_loss : 0.02\n",
      "[366/10000] mean_loss : 0.02\n",
      "[367/10000] mean_loss : 0.02\n",
      "[368/10000] mean_loss : 0.02\n",
      "[369/10000] mean_loss : 0.02\n",
      "[370/10000] mean_loss : 0.02\n",
      "[371/10000] mean_loss : 0.02\n",
      "[372/10000] mean_loss : 0.02\n",
      "[373/10000] mean_loss : 0.02\n",
      "[374/10000] mean_loss : 0.02\n",
      "[375/10000] mean_loss : 0.02\n",
      "[376/10000] mean_loss : 0.01\n",
      "[377/10000] mean_loss : 0.02\n",
      "[378/10000] mean_loss : 0.01\n",
      "[379/10000] mean_loss : 0.01\n",
      "[380/10000] mean_loss : 0.02\n",
      "[381/10000] mean_loss : 0.01\n",
      "[382/10000] mean_loss : 0.01\n",
      "[383/10000] mean_loss : 0.01\n",
      "[384/10000] mean_loss : 0.01\n",
      "[385/10000] mean_loss : 0.01\n",
      "[386/10000] mean_loss : 0.01\n",
      "[387/10000] mean_loss : 0.01\n",
      "[388/10000] mean_loss : 0.01\n",
      "[389/10000] mean_loss : 0.01\n",
      "[390/10000] mean_loss : 0.01\n",
      "[391/10000] mean_loss : 0.01\n",
      "[392/10000] mean_loss : 0.01\n",
      "[393/10000] mean_loss : 0.01\n",
      "[394/10000] mean_loss : 0.01\n",
      "[395/10000] mean_loss : 0.01\n",
      "[396/10000] mean_loss : 0.01\n",
      "[397/10000] mean_loss : 0.01\n",
      "[398/10000] mean_loss : 0.01\n",
      "[399/10000] mean_loss : 0.01\n",
      "[400/10000] mean_loss : 0.01\n",
      "[401/10000] mean_loss : 0.01\n",
      "[402/10000] mean_loss : 0.01\n",
      "[403/10000] mean_loss : 0.01\n",
      "[404/10000] mean_loss : 0.01\n",
      "[405/10000] mean_loss : 0.01\n",
      "[406/10000] mean_loss : 0.01\n",
      "[407/10000] mean_loss : 0.01\n",
      "[408/10000] mean_loss : 0.01\n",
      "[409/10000] mean_loss : 0.01\n",
      "[410/10000] mean_loss : 0.01\n",
      "[411/10000] mean_loss : 0.01\n",
      "[412/10000] mean_loss : 0.01\n",
      "[413/10000] mean_loss : 0.01\n",
      "[414/10000] mean_loss : 0.01\n",
      "[415/10000] mean_loss : 0.01\n",
      "[416/10000] mean_loss : 0.01\n",
      "[417/10000] mean_loss : 0.01\n",
      "[418/10000] mean_loss : 0.01\n",
      "[419/10000] mean_loss : 0.01\n",
      "[420/10000] mean_loss : 0.01\n",
      "[421/10000] mean_loss : 0.01\n",
      "[422/10000] mean_loss : 0.01\n",
      "[423/10000] mean_loss : 0.01\n",
      "[424/10000] mean_loss : 0.01\n",
      "[425/10000] mean_loss : 0.01\n",
      "[426/10000] mean_loss : 0.01\n",
      "[427/10000] mean_loss : 0.01\n",
      "[428/10000] mean_loss : 0.01\n",
      "[429/10000] mean_loss : 0.01\n",
      "[430/10000] mean_loss : 0.01\n",
      "[431/10000] mean_loss : 0.01\n",
      "[432/10000] mean_loss : 0.01\n",
      "[433/10000] mean_loss : 0.01\n",
      "[434/10000] mean_loss : 0.01\n",
      "[435/10000] mean_loss : 0.01\n",
      "[436/10000] mean_loss : 0.01\n",
      "[437/10000] mean_loss : 0.01\n",
      "Early Stopping!\n"
     ]
    }
   ],
   "source": [
    "# 모델 training\n",
    "for epoch in range(EPOCH):\n",
    "    losses = []\n",
    "    if EARLY_STOPPING:\n",
    "        #torch.save(model.state_dict(), 'C:/Users/82104/Desktop/데이터/모델 데이터/model/[0601]crayon.pth')\n",
    "        break\n",
    "        \n",
    "    for i,batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "        facts, fact_masks, questions, question_masks, answers = pad_to_batch(batch, word2index)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        pred = model(facts, fact_masks, questions, question_masks, answers.size(1), NUM_EPISODE, True)\n",
    "        loss = loss_function(pred, answers.view(-1))\n",
    "        #losses.append(loss.data.tolist()[0])\n",
    "        losses.append(loss.data)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"[%d/%d] mean_loss : %0.2f\" %(epoch, EPOCH, np.mean(losses)))\n",
    "            \n",
    "            if np.mean(losses) < 0.01:\n",
    "                EARLY_STOPPING = True\n",
    "                print(\"Early Stopping!\")\n",
    "                break\n",
    "            losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_fact(fact, x_to_ix): # this is for inference\n",
    "    \n",
    "    max_x = max([s.size(1) for s in fact])\n",
    "    x_p = []\n",
    "    for i in range(len(fact)):\n",
    "        if fact[i].size(1) < max_x:\n",
    "            x_p.append(torch.cat([fact[i], Variable(LongTensor([x_to_ix['<PAD>']] * (max_x - fact[i].size(1)))).view(1, -1)], 1))\n",
    "        else:\n",
    "            x_p.append(fact[i])\n",
    "        \n",
    "    fact = torch.cat(x_p)\n",
    "    fact_mask = torch.cat([Variable(ByteTensor(tuple(map(lambda s: s ==0, t.data))), volatile=False) for t in fact]).view(fact.size(0), -1)\n",
    "    return fact, fact_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test data 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = './data/test/[TEST]교양교과목안내.txt'\n",
    "test_data = bAbI_data_load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in test_data:\n",
    "    for i, fact in enumerate(t[0]):\n",
    "        t[0][i] = prepare_sequence(fact, word2index).view(1, -1)\n",
    "    \n",
    "    t[1] = prepare_sequence(t[1], word2index).view(1, -1)\n",
    "    t[2] = prepare_sequence(t[2], word2index).view(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 정확도 계산 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "for t in test_data:\n",
    "    fact, fact_mask = pad_to_fact(t[0], word2index)\n",
    "    question = t[1]\n",
    "    question_mask = Variable(ByteTensor([0] * t[1].size(1)), volatile=False).unsqueeze(0)\n",
    "    answer = t[2].squeeze(0)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    pred = model([fact], [fact_mask], question, question_mask, answer.size(0), NUM_EPISODE)\n",
    "    if pred.max(1)[1].data.tolist() == answer.data.tolist():\n",
    "        accuracy += 1\n",
    "    #print(\"\")\n",
    "    #print(\"Question : \",' '.join(list(map(lambda x: index2word[x], question.data.tolist()[0]))))\n",
    "    #print(\"\")\n",
    "    #print(\"Answer : \",' '.join(list(map(lambda x: index2word[x], answer.data.tolist()))))\n",
    "    #print(\"Prediction : \",' '.join(list(map(lambda x: index2word[x], pred.max(1)[1].data.tolist()))))\n",
    "\n",
    "print(accuracy/len(test_data) * 100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PATH = 'C:/Users/82104/Desktop/데이터/모델 데이터/model/[0601]crayon.pt'\n",
    "torch.save(model, PATH)\n",
    "torch.save(model.state_dict(), 'C:/Users/82104/Desktop/데이터/모델 데이터/model/[0601]crayon.pth')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.save({'state_dict': model.state_dict()}, 'tmp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample test\n",
    "t = random.choice(test_data)\n",
    "fact, fact_mask = pad_to_fact(t[0], word2index)\n",
    "question = t[1]\n",
    "question_mask = Variable(ByteTensor([0] * t[1].size(1)), volatile=False).unsqueeze(0)\n",
    "answer = t[2].squeeze(0)\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "pred = model([fact], [fact_mask], question, question_mask, answer.size(0), NUM_EPISODE)\n",
    "\n",
    "print(\"Facts : \")\n",
    "print('\\n'.join([' '.join(list(map(lambda x: index2word[x],f))) for f in fact.data.tolist()]))\n",
    "print(\"\")\n",
    "print(\"Question : \",' '.join(list(map(lambda x: index2word[x], question.data.tolist()[0]))))\n",
    "print(\"\")\n",
    "print(\"Answer : \",' '.join(list(map(lambda x: index2word[x], answer.data.tolist()))))\n",
    "print(\"Prediction : \",' '.join(list(map(lambda x: index2word[x], pred.max(1)[1].data.tolist()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "embed.weight \t torch.Size([84, 80])\n",
      "input_gru.weight_ih_l0 \t torch.Size([240, 80])\n",
      "input_gru.weight_hh_l0 \t torch.Size([240, 80])\n",
      "input_gru.bias_ih_l0 \t torch.Size([240])\n",
      "input_gru.bias_hh_l0 \t torch.Size([240])\n",
      "question_gru.weight_ih_l0 \t torch.Size([240, 80])\n",
      "question_gru.weight_hh_l0 \t torch.Size([240, 80])\n",
      "question_gru.bias_ih_l0 \t torch.Size([240])\n",
      "question_gru.bias_hh_l0 \t torch.Size([240])\n",
      "gate.0.weight \t torch.Size([80, 320])\n",
      "gate.0.bias \t torch.Size([80])\n",
      "gate.2.weight \t torch.Size([1, 80])\n",
      "gate.2.bias \t torch.Size([1])\n",
      "attention_grucell.weight_ih \t torch.Size([240, 80])\n",
      "attention_grucell.weight_hh \t torch.Size([240, 80])\n",
      "attention_grucell.bias_ih \t torch.Size([240])\n",
      "attention_grucell.bias_hh \t torch.Size([240])\n",
      "memory_grucell.weight_ih \t torch.Size([240, 80])\n",
      "memory_grucell.weight_hh \t torch.Size([240, 80])\n",
      "memory_grucell.bias_ih \t torch.Size([240])\n",
      "memory_grucell.bias_hh \t torch.Size([240])\n",
      "answer_grucell.weight_ih \t torch.Size([240, 160])\n",
      "answer_grucell.weight_hh \t torch.Size([240, 80])\n",
      "answer_grucell.bias_ih \t torch.Size([240])\n",
      "answer_grucell.bias_hh \t torch.Size([240])\n",
      "answer_fc.weight \t torch.Size([84, 80])\n",
      "answer_fc.bias \t torch.Size([84])\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [2000278344512, 2000278344832, 2000278344704, 2000278344000, 2000278343744, 2000277921088, 2000277920832, 2000277921152, 2000277839424, 2000278014464, 2000278014208, 2000278014656, 2000278014976, 2000278015232, 2000278015552, 2000278015616, 2000278015488, 2000278015680, 2000278015296, 2000278015168, 2000278015104, 2000278015360, 2000278015872, 2000278015808, 2000277928512, 2000277927744, 2000277928064]}]\n"
     ]
    }
   ],
   "source": [
    "# model의 파라미터들의 사이즈 확인\n",
    "\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.save(model.state_dict(), 'C:/Users/82104/Desktop/데이터/모델 데이터/model/[0602]crayon.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model, loss_function, optimizer의 parameter등을 나누어 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82104\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:359: UserWarning: Couldn't retrieve source code for container of type DMN. It won't be checked for correctness upon loading.\n",
      "  warnings.warn(\"Couldn't retrieve source code for container of \"\n"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model': DMN(len(word2index), HIDDEN_SIZE, len(word2index)),\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'criterion_state_dict': loss_function.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, 'C:/Users/82104/Desktop/데이터/모델 데이터/model/[졸업이수학점0603]crayon.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for param in model.state_dict():\n",
    "    print(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
